import pandas as pd
import numpy as np
import random as rd
import datetime
import matplotlib.pyplot as plt
links_paths_path = 'wikispeedia_paths-and-graph/'
#wpcd_path = './wpcd/'
#plaintext_articles_path = './plaintext_articles/'


#read the paths_finished.tsv
paths_finished_df = pd.read_csv(links_paths_path + 'paths_finished.tsv', sep='\t', \
                                names = ['hashedIpAddress','timestamp', 'durationInSec', 'path', 'rating'], \
                                skiprows=15)
paths_finished_df['year'] = paths_finished_df['timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(x).year)
paths_finished_df['path'] = paths_finished_df['path'].apply(lambda x: x.split(";"))

#see whether there are null elements in the dataset
paths_finished_df.isnull().sum()


#drop the rows which have null elements in column 'hashedIpAddress'
paths_finished_df[~paths_finished_df['hashedIpAddress'].isnull()]


#Only data from 2011-2014 are retained
query_year = [2011,2012,2013,2014]
paths_finished_df = paths_finished_df[paths_finished_df['year'].isin(query_year)] 


#read the paths_unfinished.tsv
paths_unfinished_df = pd.read_csv(links_paths_path + 'paths_unfinished.tsv', sep='\t',\
                                  names = ['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'target','type'], \
                                  skiprows=17)
paths_unfinished_df['year'] = paths_unfinished_df['timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(x).year)
paths_unfinished_df['path'] = paths_unfinished_df['path'].apply(lambda x: x.split(";"))

#see whether there are null elements in the dataset
paths_finished_df.isnull().sum()


#Only data from 2011-2014 are retained
query_year = [2011,2012,2013,2014]
paths_unfinished_df = paths_unfinished_df[paths_unfinished_df['year'].isin(query_year)] 


paths_df = pd.concat([paths_finished_df,paths_unfinished_df])
paths_df


paths_df.groupby('year').agg('count')


#split the strings in the column 'path' and transform them into lists
path_list_2011 = paths_df['path'][paths_df['year'] == 2011]
path_list_2012 = paths_df['path'][paths_df['year'] == 2012]
path_list_2013 = paths_df['path'][paths_df['year'] == 2013]
# path_list_2014 = paths_finished_df['path'][paths_finished_df['year'] == 2014].apply(lambda x: x.split(";"))


from collections import Counter
import wordcloud

def word_cnt(wlist):
    '''
    Count the number of occurrences of each term in all paths
    '''
    word_counts = Counter()
    
    for line in wlist:
        word_counts.update(line)
    
    return sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

def pic_words(list_):
    '''
    Draw the wordcloud plot to have a visual representation of how often they appear
    '''
    words_dict = {}
    for idx,item in enumerate(list_):
        words_dict[item[0]] = item[1]
    
    cloudobj = wordcloud.WordCloud(
        font_path='/System/Library/Fonts/menlo.ttc',
        height = 800, width = 1200, mode = "RGBA", background_color = 'white', colormap='magma'
                                  ).fit_words(words_dict)
    plt.rcParams['figure.figsize'] = (12.0, 8.0)    
    plt.imshow(cloudobj)
    plt.axis("off")
    plt.show()

#Eliminate the most high frequent word "<", take [1:51]   
counter_list_2011 = word_cnt(path_list_2011)
TOP100_2011 = counter_list_2011[1:101]

pic_words(TOP100_2011)
TOP100_2011


counter_list_2012 = word_cnt(path_list_2012)
TOP100_2012 = counter_list_2012[1:101]
pic_words(TOP100_2012)
TOP100_2012


counter_list_2013 = word_cnt(path_list_2013)
TOP100_2013 = counter_list_2013[1:101]
pic_words(TOP100_2013)
TOP100_2013


list_2011 = []
list_2012 = []
list_2013 = []

for idx,item in enumerate(TOP100_2011):
    list_2011.append(item[0])
    
for idx,item in enumerate(TOP100_2012):
    list_2012.append(item[0])
    
for idx,item in enumerate(TOP100_2013):
    list_2013.append(item[0])
    
#Calculation of words appearing in the TOP100 for all three years
print(len(list(set(list_2011).intersection(set(list_2012)).intersection(list_2013))))


import os
import sys
import utils
import gensim
import sklearn
import numpy as np
from gensim.models.doc2vec import Doc2Vec
TaggededDocument = gensim.models.doc2vec.TaggedDocument
from nltk import word_tokenize

def cut_drop_stop_words(content):
    '''
    Tokenize the doc to words
    Get the stop words from txt and drop them form articles
    :return: a cut word list
    '''
    
    stop_words = []
    with open('./en_stopwords.txt', 'r', encoding="latin-1") as f_reader:
        for line in f_reader:
            line = line.replace("\r", "").replace("\n", "")
            stop_words.append(line)
    stop_words.append('●')
    stop_words.append(',')
    stop_words = set(stop_words)
    
    if content != '' and content is not None:
        seg_list = word_tokenize(content)
        each_split = ' '.join(seg_list).split()
        each_result = [word for word in each_split if word not in stop_words] #drop stop words
        result = ' '.join(each_result)
    return result

def get_file_data_to_a_list(file_list):
    '''
    Read all article paths into a list of file paths, and read the data in each file in turn, saving all data into one file
    '''
    file_paths = []
    data = []
    for idx,item in enumerate(file_list):
        file_paths.append('./plaintext_articles/' + item + '.txt')
    
    for filename in file_paths:
        f = open(filename,'r',encoding="latin-1")
        doc = f.read().replace("\t", "").replace("\n", "")
        data.append(cut_drop_stop_words(doc))

    return data

def get_cut_dataset(data):
    '''
    Generate corpus(using tokenized word list)
    '''
    corpus = []
    documents = []

    for idx, item in enumerate(data):
        text = list(item.replace('\n', '').split(' '))
        # print(text)
        document = TaggededDocument(text, tags=[idx])
        corpus.append(document)
    print('len of corpus：', len(corpus))

    return corpus


def train(x_train, model_path , size=200, epoch_num=20, dm=1):
    print('start train')
    model_dm = Doc2Vec(x_train, min_count=10, window=5, vector_size=size, sample=1e-3, negative=5, workers=4, dm=dm)
    model_dm.train(x_train, total_examples=model_dm.corpus_count, epochs=epoch_num)
    model_dm.save(model_path)
    print('end train')
    return model_dm


def test_(model_path, str):
    model_dm = Doc2Vec.load(model_path)
    test_text = ' '.join(word_tokenize(str)).split(' ')
    inferred_vector_dm = model_dm.infer_vector(test_text)
    print('inferred_vector_dm:', inferred_vector_dm)
    sims = model_dm.dv.most_similar([inferred_vector_dm], topn=1)
    return sims

# Train model for 2011 TOP100
model_path = './model_2011'
temp = get_file_data_to_a_list(list_2011)
train_corpus = get_cut_dataset(temp)
model_dm_2011 = train(train_corpus, model_path=model_path)

# Use an article to test whether that the most similar one is itself
f = open('./plaintext_articles/United_States.txt','r',encoding="latin-1")
test = f.read().replace("\t", "").replace("\n", "")

sims = test_(model_path, test)
for count,sim in sims:
    sentence = train_corpus[count]
    words = ''
    for word in sentence[0][:200]:
        words = words + word + ' '
    print(words, sim)



import matplotlib.pyplot as plt
import seaborn as sns
import torch


model_dm = Doc2Vec.load(model_path)
matrix = model_dm[list(range(100))]

# Calculate cosine similarity matrix of top articles to represent their semantic similarity
cos_sim = np.zeros((100,100))
for i in range(100):
    for j in range(100):
        cos_sim[i,j] = matrix[i].dot(matrix[j]) / (np.linalg.norm(matrix[i]) * np.linalg.norm(matrix[j]))

# heatmap of Cosine Similarity Matrix 
mask = np.zeros_like(cos_sim, dtype = bool)
mask[np.triu_indices_from(mask)] = True

fig = plt.figure(figsize = (14,14))
ax = fig.add_subplot()
sns.heatmap(np.abs(cos_sim), ax=ax, mask = mask, cmap="YlGnBu", square=True, fmt='.3g',\
             vmax=1, vmin=0 , annot=False, cbar=True,cbar_kws={"shrink": 0.8})
plt.title("Cosine Similarity Matrix of 2011 TOP100 Articles", fontsize = 20)
plt.axis('off')
plt.savefig('SimMatrix2011.png', bbox_inches='tight')

cos_sim


def distance_to_end(TOPlist, PATHlist):
    '''
    create a dictionary that records the first appearing position to the end of every article name in paths df
    '''
    position_to_end_dict = {}
    k = 0
    for article_name in TOPlist:
        position_to_end_dict[article_name] = []
        for path in PATHlist:
            if article_name in path:
                length = len(path) #the length of the whole path
                index = path.index(article_name) #the first position where the article_name appears
                position_to_end = length - index - 1 #the distance between the position and the end
                position_to_end_dict[article_name].append(position_to_end)
            else:
                #for paths where the article_name does not exist, use nan to fill in the blank
                position_to_end_dict[article_name].append(np.nan)
        k = k + 1  
    return position_to_end_dict


finished_distance_dict_2011 = distance_to_end(list_2011, paths_finished_df['path'][paths_finished_df['year'] == 2011])
position_to_end_pd_finished_2011 = pd.DataFrame(finished_distance_dict_2011)
position_to_end_pd_finished_2011


unfinished_distance_dict_2011 = distance_to_end(list_2011, paths_unfinished_df['path'][paths_unfinished_df['year'] == 2011])
position_to_end_pd_unfinished_2011 = pd.DataFrame(unfinished_distance_dict_2011)
position_to_end_pd_unfinished_2011
# 0.0 means it is the last point in the path


def position_to_end_avg(distance_dict):
    '''
    Compute the average of the distance between all the article_names and their ends
    and the rate that the path finishes in 3 words after the first appearance of article_name
    '''
    position_to_end_avg = {}
    for article_name in distance_dict:
        total_num = 0 #the total number of the times article_name appears
        goal_num = 0 #the number of the times the position_to_end of article_name is smaller than 3
        total_dist_num = 0 #the sum of the distance between all the artilce_names and their ends
        for distance in distance_dict[article_name]:
            if distance > -1:
                if distance < 3:
                    goal_num += 1
                total_num += 1
                total_dist_num += distance
        avg_dist = total_dist_num / total_num #the average of the distance between all the article_names and their ends
        avg_rate = goal_num / total_num #the rate that the path finishes in 3 words after the first appearance of article_name
        position_to_end_avg[article_name] = [avg_dist, avg_rate]
    return position_to_end_avg


position_to_end_avg_finished_2011 = position_to_end_avg(finished_distance_dict_2011)
position_to_end_avg_finished_2011


position_to_end_avg_unfinished_2011 = position_to_end_avg(unfinished_distance_dict_2011)
position_to_end_avg_unfinished_2011


def avg_duration(TOPlist,PATHdict):
    '''
    Compute the average duration time of article_names in toplist
    '''
    top_time_dict = {}
    for article_name in TOPlist:
        time_dura_sum = 0
        times = 0
        for i in PATHdict:
            if article_name in PATHdict[i]['path']:
                time_dura_sum += PATHdict[i]['durationInSec']
                times += 1
        time_avg = time_dura_sum / times
        top_time_dict[article_name] = time_avg
    return top_time_dict

top100_finished_time_dict_2011 = avg_duration(list_2011,paths_finished_df[paths_finished_df['year']==2011].to_dict('index'))
top100_finished_time_dict_2011


top100_unfinished_time_dict_2011 = avg_duration(list_2011,paths_unfinished_df[paths_unfinished_df['year']==2011].to_dict('index'))
top100_unfinished_time_dict_2011


def failtype(TOPlist,PATHdict):
    '''
    Count the times of the appearances of timeout times with ratio & restart times with ratio in top100_list
    '''
    failure_type_dict = {}
    for idx,article in enumerate(TOPlist):
        timeout_sum = 0
        restart_sum = 0
        for i in PATHdict:
            if article[0] in PATHdict[i]['path']:
                if PATHdict[i]['type'] == 'timeout':
                    timeout_sum += 1
                else:
                    restart_sum += 1
        timeout_ratio = timeout_sum/article[1]
        restart_ratio = restart_sum/article[1]
        failure_type_dict[article[0]] = [timeout_sum, timeout_ratio, restart_sum, restart_ratio]
    return failure_type_dict

top100_failure_type_dict_2011 = failtype(TOP100_2011,paths_unfinished_df[paths_unfinished_df['year']==2011].to_dict('index'))
top100_failure_type_dict_2011


# Read the articles' names in the articles.tsv
articles_df = pd.read_csv(links_paths_path + 'articles.tsv', sep='\t',\
                                  names = ['article'],  skiprows=12)


# Generate a random article list which has the same size as TOP list as control group
random_data = articles_df.sample(n=100)
random_list = np.array(random_data).tolist()
random_list


categories_df = pd.read_csv(links_paths_path + 'categories.tsv', sep='\t',\
                                  names = ['article', 'category'],  skiprows=13)


countries = categories_df[categories_df['category'] == 'subject.Countries'].reset_index()
countries


categories_df = categories_df[~(categories_df['category'] == 'subject.Countries')].reset_index(drop=True)
categories_df


# Take the penultimate category of the hierarchy
temp_df = categories_df['category'].copy()
categories_df['category'] = temp_df.apply(lambda x: x.split("."))
temp_df = categories_df['category'].copy()
categories_df['category'] = temp_df.apply(lambda x: x[-2])
categories_df['category'].unique()


# Fix the hierarchy so that all levels are at the same semantic level
categories_df['category'][categories_df['category'] == 'British_History'] = 'History'
geo = ['African_Geography','Geography_of_Asia','Geography_of_the_Middle_East','Central_and_South_American_Geography','European_Geography']
categories_df['category'][categories_df['category'].isin(geo)] = 'Geography'
categories_df['category'].unique()             


# Drop duplicates
categories_df = categories_df.drop_duplicates().reset_index(drop=True)
categories_df


#Read the GDP database
GDP_df = pd.read_csv('GDP_world_bank')
GDP_df = GDP_df.dropna()
GDP_df.head(5)


# Only data of 2012,2013,2014 are retained
GDP_df = GDP_df.iloc[:,[2,6,7,8]]
GDP_df.rename(columns={'2012 [YR2012]': '2012', '2013 [YR2013]': '2013', '2014 [YR2014]': '2014'}, inplace=True)

# Harmonisation of the expression of the name of the country
GDP_df = GDP_df.replace('Russian Federation','Russia')
GDP_df = GDP_df.replace('Egypt, Arab Rep.','Egypt')
GDP_df


# Integration of country names, hotness and GDP into one dict
countries_dict = {}
countries_list = countries['article'].tolist()
for idx,item in enumerate(TOP100_2012):
    if item[0] in countries_list:
        gdp = float(GDP_df['2012'][GDP_df['Country Name'] == item[0].replace('_', ' ')].max())
        countries_dict[item[0]] = [item[1],gdp]
countries_dict


countries_dict['China'][0] = countries_dict['China'][0] + countries_dict['People%27s_Republic_of_China'][0]
del countries_dict['People%27s_Republic_of_China']
countries_dict


from matplotlib import pyplot as plt

country = np.array([])
hot = np.array([])
gdp_array = np.array([])
for key,value in countries_dict.items():
    country = np.append(country, key)
    hot = np.append(hot,value[0])
    gdp_array = np.append(gdp_array,value[1])

fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(country, hot, linestyle=':', label = 'Hotness',marker='+')

ax2 = ax.twinx()
ax2.plot(country, gdp_array, color = 'green', label = 'GDP', marker='*')
fig.legend(loc=1, bbox_to_anchor=(1,1), bbox_transform=ax.transAxes)
fig.autofmt_xdate()

plt.show



