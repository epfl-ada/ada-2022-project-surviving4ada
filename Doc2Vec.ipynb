{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b1cc11-291c-41f6-91ff-e6d55151ac98",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "Obtain embedding representation of articles in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3943b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import utils\n",
    "import gensim\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "TaggededDocument = gensim.models.doc2vec.TaggedDocument\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9df6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_drop_stop_words(content):\n",
    "    '''\n",
    "    Tokenize the doc to words\n",
    "    Get the stop words from txt and drop them form articles\n",
    "    :return: a cut word list\n",
    "    '''\n",
    "    \n",
    "    stop_words = []\n",
    "    with open('./en_stopwords.txt', 'r', encoding=\"latin-1\") as f_reader:\n",
    "        for line in f_reader:\n",
    "            line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "            stop_words.append(line)\n",
    "    stop_words.append('●')\n",
    "    stop_words.append(',')\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    if content != '' and content is not None:\n",
    "        seg_list = word_tokenize(content)\n",
    "        each_split = ' '.join(seg_list).split()\n",
    "        each_result = [word for word in each_split if word not in stop_words] #drop stop words\n",
    "        result = ' '.join(each_result)\n",
    "    return result\n",
    "\n",
    "def get_file_data_to_a_list(file_list):\n",
    "    '''\n",
    "    Read all article paths into a list of file paths, and read the data in each file in turn, saving all data into one file\n",
    "    '''\n",
    "    file_paths = []\n",
    "    data = []\n",
    "    for idx,item in enumerate(file_list):\n",
    "        file_paths.append('./plaintext_articles/' + item + '.txt')\n",
    "    \n",
    "    for filename in file_paths:\n",
    "        f = open(filename,'r',encoding=\"latin-1\")\n",
    "        doc = f.read().replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "        data.append(cut_drop_stop_words(doc))\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_cut_dataset(data):\n",
    "    '''\n",
    "    Generate corpus(using tokenized word list)\n",
    "    '''\n",
    "    corpus = []\n",
    "    documents = []\n",
    "\n",
    "    for idx, item in enumerate(data):\n",
    "        text = list(item.replace('\\n', '').split(' '))\n",
    "        # print(text)\n",
    "        document = TaggededDocument(text, tags=[idx])\n",
    "        corpus.append(document)\n",
    "    print('len of corpus：', len(corpus))\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def train(x_train, model_path , size=300, epoch_num=20, dm=1):\n",
    "    print('start train')\n",
    "    model_dm = Doc2Vec(x_train, min_count=10, window=5, vector_size=size, sample=1e-3, negative=5, workers=4, dm=dm)\n",
    "    model_dm.train(x_train, total_examples=model_dm.corpus_count, epochs=epoch_num)\n",
    "    model_dm.save(model_path)\n",
    "    print('end train')\n",
    "    return model_dm\n",
    "\n",
    "\n",
    "def test_(model_path, str):\n",
    "    model_dm = Doc2Vec.load(model_path)\n",
    "    test_text = ' '.join(word_tokenize(str)).split(' ')\n",
    "    inferred_vector_dm = model_dm.infer_vector(test_text)\n",
    "    print('inferred_vector_dm:', inferred_vector_dm)\n",
    "    sims = model_dm.dv.most_similar([inferred_vector_dm], topn=1)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fffca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_paths_path = 'wikispeedia_paths-and-graph/'\n",
    "articles_df = pd.read_csv(links_paths_path + 'articles.tsv', sep='\\t',\\\n",
    "                                  names = ['article'],  skiprows=12)\n",
    "articles_list = articles_df['article'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c3723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of corpus： 4604\n",
      "start train\n",
      "end train\n"
     ]
    }
   ],
   "source": [
    "model_path = './article2vec'\n",
    "temp = get_file_data_to_a_list(articles_list)\n",
    "train_corpus = get_cut_dataset(temp)\n",
    "model_ = train(train_corpus, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666a0817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred_vector_dm: [-2.1838787  -0.48399806  0.15809946 -0.14729244  0.34973332 -1.4335711\n",
      " -0.4967516   1.5664557  -1.0084097   1.0260029  -0.50888044 -0.06240981\n",
      " -1.6091268   0.9134005  -1.97776    -1.0039107  -0.50500184  0.8283831\n",
      "  1.4146003  -1.169062    0.16794592 -1.0245419   0.62139326 -1.4466877\n",
      " -1.1098875   0.8930133  -1.6181768  -0.8313622  -1.494734    0.3323823\n",
      " -0.12109297 -1.0231631  -2.0349224  -1.038635   -1.0756977  -0.711748\n",
      " -0.9778016   0.5457843   1.5275923  -2.4995084  -0.9455212  -1.1861526\n",
      "  0.4635846   1.4584116   1.3225995  -1.0851151  -1.101253   -0.24172391\n",
      "  0.82580477 -1.0221933  -1.0289146   2.6091213  -0.21463059 -0.89533377\n",
      "  1.9647735   0.53621775 -0.03462055 -0.18051496 -0.5938423  -0.74684364\n",
      "  0.42481962  0.0447261  -0.56526184  0.39519164  0.78881365 -0.5501906\n",
      " -0.8492291  -1.2902513   2.1995742  -0.92629135 -0.893451   -0.62538654\n",
      "  0.72446316  0.4102887  -1.0334884  -0.926342    0.23691113 -1.5584357\n",
      "  0.44277218  1.8878403  -0.5449402  -0.8587438  -1.4942775   0.6373624\n",
      " -0.5875716   1.1658845  -0.12664624  1.2571266   0.88958997 -0.41191193\n",
      "  1.6851871  -0.29858318  0.5026448  -1.6348993   0.20592749  2.1196675\n",
      "  0.12463716  0.20612958 -0.2420025  -0.48762697  1.4882873   0.06106778\n",
      " -1.7929949  -1.4820309  -0.7860958   0.18163575 -1.8702044   1.4739224\n",
      "  0.902855   -2.5106623   0.10355235 -0.21822776 -0.68271     0.7105844\n",
      " -1.6874193  -0.065786    0.41026476 -0.81034136  1.5575063  -0.12068056\n",
      "  0.30776802  0.75744295  1.8481568   1.8623415   0.376406   -0.76003504\n",
      "  0.31506592  0.34880796 -1.8712012  -1.2391217  -0.6627594  -0.6383779\n",
      " -0.90839916 -0.8825532   0.36618078  2.4925642   0.5478214   0.4266905\n",
      "  1.0771451  -0.06361156  0.5283878  -0.23729426  1.1876018   0.21082155\n",
      " -0.54487646  1.3862942   0.8518453  -0.89622056 -0.77658474 -0.24461414\n",
      " -1.5179724   0.9399358  -1.0054551  -0.00490403 -2.524517    2.0933213\n",
      " -0.68120795  1.3176751  -0.9306743  -0.04833945 -2.445894   -0.23512524\n",
      " -1.2980305   0.61246055  0.60138196  1.0708941  -1.6017432   1.3135912\n",
      "  0.6924775   0.41054034 -0.10109771 -1.7556379   0.31673923 -0.58211637\n",
      "  0.20543039 -1.8054736   0.08103683  1.914043   -0.55346537  0.25813878\n",
      " -1.1773756   1.5991709  -1.297883    1.3589941  -0.15108125 -0.8600612\n",
      " -1.0589566  -0.16771317 -1.9503924   1.2368752  -0.10015196  0.01800218\n",
      "  0.17003007  1.1364596  -0.64432186 -0.67371845  1.3930478  -2.2985947\n",
      " -0.41272962  1.0197686  -0.82223606  2.324025   -0.63693404 -1.6812382\n",
      "  2.7421196  -1.8731772   0.2773442   1.710843    0.4939598   0.25368342\n",
      "  0.69876134  0.15589514  0.8221164  -2.183537    1.7039335   1.3160185\n",
      "  0.12195545  0.3459211  -1.3261886  -3.3624542  -1.5751173  -0.13012569\n",
      "  0.4156281  -0.14023638 -0.17579892  1.1838477   0.72283393  0.9325996\n",
      " -2.088897   -1.5272752  -0.69987875  0.35353312  0.8720764  -1.7714585\n",
      " -2.0502768   0.83720624  1.3600589  -0.1918971  -0.7539225   1.008566\n",
      " -0.92627645 -1.6033254   1.6606117  -1.5026708   0.10150941  0.69379807\n",
      " -0.43618754 -0.01246523 -1.2264704  -0.28032643  1.0592132   1.1857557\n",
      " -1.0031936  -0.8989874   2.103874    1.36244    -0.62831527 -0.18584085\n",
      " -1.2271272  -0.09895431 -1.3867263   1.588121   -0.257454    0.3756146\n",
      " -0.51567084  1.7164975  -1.4026879   0.50023293 -0.04952451 -0.88027894\n",
      " -0.33925012  0.88519555 -1.3379601   0.743946    0.63290983  0.18531142\n",
      " -1.217977   -0.870508   -2.505174    0.6140274   0.92751426  2.0870478\n",
      " -1.2197435   1.6578593  -1.2849814  -0.9955959  -2.591785   -0.15848002\n",
      " -0.6449045   0.7467848  -1.6224244  -0.39656758  0.4739667   0.81329167\n",
      "  0.7966842   1.562784    0.02963067 -0.7155402  -0.2685175  -1.1362014 ]\n",
      "copyrightUnited States2007 Schools Wikipedia Selection Related subjects Countries North AmericanGeography United States America Flag United States Great Seal United States Flag Great Seal Motto E Pluribus Unum `` Out Of Many One '' traditional In God We Trust 1956 Anthem The Star-Spangled Banner Location United States Capital Washington D.C. 38Â°53â²N 77Â°02â²W Largest city New York City Official languages None federal level English de facto Government Federal republic President George Walker Bush R Vice President Dick Cheney R Independence Great Britain Declared July 4 1776 Recognized September 3 1783 Area Total 9,631,420 kmÂ² 3rd^1 3,718,695 sq mi Water 4.87 Population 2006 estimate 300,307,297 3rd 2000 census 281,421,906 Density 31/kmÂ² 172nd 80/sq mi GDP PPP 2006 estimate Total 13.049 trillion 1st Per capita 43,555 3rd GDP nominal 2005 estimate Total 12.485 trillion 1st Per capita 42,000 8th HDI 2004 0.948 8th Currency United States dollar USD Time zone UTC-5 -10 Summer DST UTC-4 -10 Internet TLD .us .gov .edu .mil .um Calling code +1 ^1 Sometimes listed 4th rank disputed China Washington Montana N. Dakota S. Dakota Nebraska Oregon Idaho Wyoming Colorado California Nevada Utah Florida Texas Louisiana MS AL GA TN NC SC VA Oklahoma Arkansas Missouri Iowa MN IL IN  0.9837670922279358\n"
     ]
    }
   ],
   "source": [
    "# Use an article to test whether that the most similar one is itself\n",
    "f = open('./plaintext_articles/United_States.txt','r',encoding=\"latin-1\")\n",
    "test = f.read().replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "sims = test_(model_path, test)\n",
    "for count,sim in sims:\n",
    "    sentence = train_corpus[count]\n",
    "    words = ''\n",
    "    for word in sentence[0][:200]:\n",
    "        words = words + word + ' '\n",
    "    print(words, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e409f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./plaintext_articles\" \n",
    "files= os.listdir(path) \n",
    "txts = []\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fb02910",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files: \n",
    "    if file != '.ipynb_checkpoints':\n",
    "        position = path+'/'+ file \n",
    "        with open(position, \"r\",encoding='utf-8') as f:    \n",
    "            data = f.read()   \n",
    "            txts.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc8291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to ./Doc2VecArray\n"
     ]
    }
   ],
   "source": [
    "data = np.zeros((len(txts),model_.vector_size))\n",
    "for i,item in enumerate(txts):\n",
    "    text = list(item.split(' ') if type(item) == str else str(item))\n",
    "    data[i] = model_.infer_vector(text)\n",
    "\n",
    "np.save(\"./Doc2VecArray\", data)\n",
    "\n",
    "print(\"save to ./Doc2VecArray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad146071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
